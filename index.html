<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiang Li</title>
    <meta content="Xiang Li, https://implus.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h3 {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Verdana, Helvetica, sans-serif;
            font-size: 13px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 130px;">
    <div style="margin: 0px auto; width: 100%;">
        <img title="implus" style="float: left; padding-left: .01em; height: 130px;"
             src="./resources/images/me.png">
        <div style="padding-left: 12em; vertical-align: top; height: 120px;">
            <span style="line-height: 150%; font-size: 20pt;">Xiang Li (李翔)</span><br>
            <span> <a href="http://cs.njust.edu.cn/">Department of Computer Science and Technology, Nanjing University of Science and Technology (NJUST)</a></span><br>
            <span><strong>Address</strong>: 200 Avenue, Xuanwu District, Nanjing, China</span><br>
            <span><strong>Email</strong>: xiang.li.implus [at] {njust.edu.cn} </span> <br>
        </div>
    </div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
    <div class="section">
        <h2>About Me [<a href="https://github.com/implus">GitHub</a>]
            [<a href="https://scholar.google.com/citations?user=oamjJdYAAAAJ&hl=zh-CN">Google Scholar</a>]
            <!--[<a href="./resources/cv/wwh_cv.pdf">CV</a>])-->
        </h2>
        <div class="paper">
            I got my PhD degree from the Department of Computer Science and Technology, Nanjing University of Science and Technology (NJUST) in 2020.
            I started my postdoctoral career in NJUST as a candidate for the <a href="https://zhuanlan.zhihu.com/p/147471409">2020 Postdoctoral Innovative Talent Program</a>.
            My advisor is <a href="http://202.119.85.163/open/TutorInfo.aspx?dsbh=tLbjVM9T1OzsoNduSpyHQg==&yxsh=4iVdgPyuKTE=&zydm=L-3Jh59wXco=">Prof. Jian Yang</a> from NJUST, who is a Changjiang Scholar. My vice-advisor is <a href="http://www.xlhu.cn/">Prof. Xiaolin Hu</a> from Tsinghua University.
            In 2016, I spent 8 months as a research intern in Microsoft Research Asia, supervised by <a href="https://scholar.google.com/citations?user=Bl4SRU0AAAAJ&hl=zh-CN">Prof. Tao Qin</a> and <a href="https://scholar.google.com/citations?user=Nh832fgAAAAJ&hl=zh-CN">Prof. Tie-Yan Liu</a>.
            I am also a long-term visiting scholar at <a href="https://www.momenta.cn/">Momenta</a>, mainly focusing on monocular perception algorithm.
            <br><br>

            My recent works are mainly on monocular perception, object detection/recognition,
            unsupervised learning, big data and neural architecture design.
            <br>
            <!-- <p style='color:red'><strong>I am looking for a postdoctoral position. Please feel free to contact me through the email.</strong></p> -->
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Honor</h2>
        <div class="paper">
            <ul>
                <li>
                    Champion of 2015 Alibaba Tianchi's first big data competition, namely Ali mobile recommendation algorithm, <strong>300,000 RMB bonus (1st from 7186 team)</strong> 
                </li>
                <li>
                    Champion of 2016 Didi Tech Di-Tech's first big data competition, namely the travel demand prediction algorithm, <strong>100,000 US dollars bonus (1st from 7664 team)</strong>
                </li>
                <li>
                    Second place of 2020 Zhengtu Cup's first AI competition, namely the industrial defect detection algorithm, <strong>150,000 RMB bonus (2st from 900 teams)</strong>
                </li>
                <li>
                    2015 Dean Medal of School of Computer Science, Nanjing University of Science and Technology, 2016 Presidential Medal of Nanjing University of Science and Technology, 2016 National Scholarship
                </li>
                <li>
                    ACM-ICPC Asia Regional Contest, Silver Medal (1st)
                </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>

<div style="clear: both;">
    <div class="section">
        <h2 id="experience">News</h2>
        <div class="paper">
            <ul>
                <li> 1 paper accepted in CVPR 2021.  </li>
                <li> 1 paper accepted in NeurIPS 2020.  </li>
                <li> 1 paper accepted in AAAI 2020.  </li>
                <li> 3 papers accepted in CVPR 2019. </li>
                <li> 1 paper accepted in CVPR 2018.  </li>
                <li> 1 paper accepted in IJCAI 2018. </li>
                <li> 1 paper accepted in NeurIPS 2016. </li>
            </ul>
            <div class="spanner"></div>
        </div>
    </div>
</div>
    


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Selected Publications</h2>
        
        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2021_GFLv2.jpg"
                                title="Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object Detection">
            <div><strong>Generalized Focal Loss V2: Learning Reliable Localization Quality Estimation for Dense Object
                Detection</strong><br>
                <strong>Xiang Li</strong>, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang<br>
                in CVPR, 2021<br>
                <a href="https://arxiv.org/pdf/2011.12885.pdf">[Paper]</a>
                <a href="https://github.com/implus/GFocalV2">[Code]</a><img
                        src="https://img.shields.io/github/stars/implus/GFocalV2?style=social"/>
                <a href="./resources/bibtex/CVPR_2021_GFLv2.txt">[BibTex]</a>
                <br>
                <alert>The improved version of GFocal!
                </alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/GFocal.png"
                                title="Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection">
            <div><strong>Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection</strong><br>
                <strong>Xiang Li</strong>*, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, Jian Yang<br>
                in NeurIPS, 2020<br>
                [<a href="https://arxiv.org/pdf/2006.04388.pdf">Paper</a>]
                [<a href="https://github.com/implus/GFocal">Code</a>]<img src="https://img.shields.io/github/stars/implus/GFocal?style=social"/>
                <br>
                <alert>We propose the generalized focal loss for learning the improved representations of dense object detector. GFocal is 
                officially included in [<a href="https://github.com/open-mmlab/mmdetection">MMDetection</a>], and is an important part of the
                [<a href="https://dy.163.com/article/FLF2LGTP0511ABV6.html">winning solution</a>] in GigaVision contest (object detection and tracking tracks)
                hosted in ECCV 2020 workshop (winner: DeepBlueAI team).
                </alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_SKNet.png"
                                title="Selective kernel networks">
            <div><strong>Selective kernel networks</strong><br>
                <strong>Xiang Li</strong>*, Wenhai Wang, Xiaolin Hu, Jian Yang<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Selective_Kernel_Networks_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2019_SKNet.txt">BibTex</a>]
                [<a href="https://github.com/implus/PytorchInsight">Code</a>]<img src="https://img.shields.io/github/stars/implus/PytorchInsight?style=social"/>
                <br>
                <alert>We propose a selective kernel mechanism for convolution.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_DIS_DROPOUT_BN.png"
                                title="Understanding the disharmony between dropout and batch normalization by variance shift">
            <div><strong>Understanding the disharmony between dropout and batch normalization by variance shift</strong><br>
                <strong>Xiang Li</strong>*, Shuo Chen, Xiaolin Hu, Jian Yang<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Understanding_the_Disharmony_Between_Dropout_and_Batch_Normalization_by_Variance_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2019_DIS_DROPOUT_BN.txt">BibTex</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We explore and address the disharmony between dropout and batch normalization.</alert>
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2020_DIS_WN_WD.png"
                                title="Understanding the disharmony between weight normalization family and weight decay">
            <div><strong>Understanding the disharmony between weight normalization family and weight decay</strong><br>
                <strong>Xiang Li</strong>*, Shuo Chen, Jian Yang<br>
                in AAAI, 2020<br>
                [<a href="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-LiX.1379.pdf">Paper</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We explore and address the disharmony between weight normalization family and weight decay.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2016_LightRNN.png"
                                title="LightRNN: Memory and computation-efficient recurrent neural networks">
            <div><strong>LightRNN: Memory and computation-efficient recurrent neural networks</strong><br>
                <strong>Xiang Li</strong>*, Tao Qin, Jian Yang, Tie-Yan Liu<br>
                in NeurIPS, 2016<br>
                [<a href="https://papers.nips.cc/paper/6512-lightrnn-memory-and-computation-efficient-recurrent-neural-networks.pdf">Paper</a>]
                [<a href="./resources/bibtex/NeurIPS_2016_LightRNN.txt">BibTex</a>]
                <!--[<a href="">Code</a>]-->
                <br>
                <alert>We propose a memory and computation-efficient recurrent neural networks for language model.</alert>
            </div>
            <div class="spanner"></div>
        </div>



        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2018_STCGAN.png"
                                title="Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal">
            <div><strong>Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal</strong><br>
                Jifeng Wang*, <strong>Xiang Li</strong>*, Jian Yang<br>
                in CVPR, 2018<br>
                [<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Stacked_Conditional_Generative_CVPR_2018_paper.pdf">Paper</a>]
                [<a href="./resources/bibtex/CVPR_2018_STCGAN.txt">BibTex</a>]
                [<a href="https://github.com/DeepInsight-PCALab/ST-CGAN">Dataset</a>]<img src="https://img.shields.io/github/stars/DeepInsight-PCALab/ST-CGAN?style=social"/>
                <br>
                <alert>We release a new dataset for jointly shadow detection and removal.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2019_PSENet.png"
                                title="Shape Robust Text Detection with Progressive Scale Expansion Network">
            <div><strong>Shape Robust Text Detection with Progressive Scale Expansion Network</strong><br>
                Wenhai Wang*, Enze Xie*, <strong>Xiang Li</strong>*, Wenbo Hou, Tong Lu, Gang Yu, Shuai Shao<br>
                in CVPR, 2019<br>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Shape_Robust_Text_Detection_With_Progressive_Scale_Expansion_Network_CVPR_2019_paper.pdf">Paper</a>]
                [<a href="./resources/posters/CVPR_2019_PSENet.pdf">Poster</a>]
                [<a href="./resources/bibtex/CVPR_2019_PSENet.txt">BibTex</a>]
                [<a href="https://github.com/whai362/PSENet">Code</a>]<img src="https://img.shields.io/github/stars/whai362/PSENet?style=social"/>
                <br>
                <alert>We proposed a segmentation-based text detector that can precisely detect text instances with arbitrary shapes.</alert>
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/IJCAI_2018_MixNet.png"
                                title="Mixed Link Networks">
            <div><strong>Mixed Link Networks</strong><br>
                Wenhai Wang*, <strong>Xiang Li</strong>*, Jian Yang, Tong Lu<br>
                in IJCAI, 2018<br>
                [<a href="https://www.ijcai.org/Proceedings/2018/0391.pdf">Paper</a>]
                [<a href="./resources/posters/IJCAI_2018_MixNet.pdf">Poster</a>]
                [<a href="./resources/bibtex/IJCAI_2018_MixNet.txt">BibTex</a>]
                [<a href="https://github.com/DeepInsight-PCALab/MixNet">Code</a>]<img src="https://img.shields.io/github/stars/DeepInsight-PCALab/MixNet?style=social"/>
                <br>
                <alert>We proposed an parameter-efficient convolutional neural networks for image classification. </alert>
            </div>
            <div class="spanner"></div>
        </div>

    </div>
</div>



<div style="clear: both;">
    <div class="section">
        <h2>Review Services</h2>
        <div class="paper">
            <strong>Journal Reviewer</strong><br>
            TNNLS
            <br>

            <strong>Conference Reviewer</strong><br>
            CVPR: 2020, 2021<br>
            AAAI: 2019, 2020, 2021<br>
        </div>
    </div>
</div>

<div style='width:600px;height:300px;margin:0 auto'>
    <!--<a href="https://clustrmaps.com/site/1b7cl" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff"></a>-->
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=wHZzsZcsBCB6HxOqmaDqDJX5oLT_yvrQ5HQx9agcvJo&cl=ffffff&w=a"></script>-->
    <!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=ItPniLpvAlUyBKssdgFtKwBDg8lP1ao3ju4dmDzw6uA&cl=ffffff&w=a"></script>-->
    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=ItPniLpvAlUyBKssdgFtKwBDg8lP1ao3ju4dmDzw6uA&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
</div>

</body>
</html>
